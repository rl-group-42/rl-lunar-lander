{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class ActorNetwork(Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self._first_layer = Dense(256, activation='relu', input_shape= state_dim)\n",
    "        self._second_layer = Dense(128, activation='relu')\n",
    "        # Output 2 dimensional continous action [-1, 1]\n",
    "        self._output_layer = Dense(action_dim[0], activation='tanh')\n",
    "        self._state_dim = state_dim\n",
    "        self._action_dim = action_dim\n",
    "\n",
    "    # Return action tensorflow with shape (1, state_dim)\n",
    "    def call(self, state):\n",
    "        first_input = self._first_layer(state)\n",
    "        second_input = self._second_layer(first_input)\n",
    "        output_action = self._output_layer(second_input)\n",
    "        return output_action\n",
    "\n",
    "class CriticNetwork(Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self._first_state_layer = Dense(64, activation='relu', input_shape=(state_dim[0] + action_dim[0],))\n",
    "        self._second_state_layer = Dense(32, activation='relu')\n",
    "        # Output a single value which is the action-value\n",
    "        self._output_layer = Dense(1, activation=None)\n",
    "    \n",
    "    # Output a value of dimension 1 shape (1, state_dim + action_dim)\n",
    "    def call(self, state_action):\n",
    "        first_state_input = self._first_state_layer(state_action)\n",
    "        second_state_input = self._second_state_layer(first_state_input)\n",
    "        output = self._output_layer(second_state_input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, max_mem_size):\n",
    "        self._max_mem_size = max_mem_size\n",
    "        self._memory = []\n",
    "        self._last_transition = ()\n",
    "        self._mem_ctr = 0\n",
    "\n",
    "    def store_transition(self, state, action, new_state, reward, terminal):\n",
    "        transition = (state, action, new_state, reward, terminal)\n",
    "        # If buffer is not full\n",
    "        if len(self._memory) < self._max_mem_size:\n",
    "            self._memory.append(transition)\n",
    "        else:\n",
    "            self._memory[self._mem_ctr] = transition\n",
    "            # Circular buffer counter\n",
    "            self._mem_ctr = (self._mem_ctr + 1) % self._max_mem_size\n",
    "\n",
    "    def store_last_transition(self,state, action, new_state, reward, terminal):\n",
    "        self._last_transition = (state, action, new_state, reward, terminal)\n",
    "\n",
    "    def sample_transition(self, batch_size):\n",
    "        last_transition = []\n",
    "        if len(self._memory) < batch_size:\n",
    "            last_transition.append(self._last_transition)\n",
    "            return last_transition\n",
    "        batch_indexes = np.random.choice(len(self._memory), batch_size, replace=False)\n",
    "        samples = [self._memory[i] for i in batch_indexes]\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckNoise:\n",
    "    def __init__(self, action_dim, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim[0]\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        state = self.state\n",
    "        dx = self.theta * (self.mu - state) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = state + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, state_dim, action_dim, gamma= 0.5, beta= 0.01,\n",
    "                 capacity=100, mem_size= 100, learning_rate= 0.1, batch_size = 5):\n",
    "        self._learning_rate = learning_rate\n",
    "        self._capacity = capacity\n",
    "        self._batch_size = batch_size\n",
    "        self._gamma = gamma\n",
    "        self._beta = beta\n",
    "        self._mem_size = mem_size\n",
    "        self._min_action = -1\n",
    "        self._max_action = 1\n",
    "\n",
    "        self._state_dim = state_dim\n",
    "        self._action_dim = action_dim\n",
    "        self._state_dim_net = state_dim[0] + action_dim[0]\n",
    "        self._noise = OrnsteinUhlenbeckNoise(action_dim)\n",
    "\n",
    "        self._replay_memory = ReplayMemory(self._mem_size)\n",
    "\n",
    "        self._actor = ActorNetwork(state_dim, action_dim)\n",
    "        self._target_actor = ActorNetwork(state_dim, action_dim)\n",
    "        self._critic = CriticNetwork(state_dim, action_dim)\n",
    "        self._target_critic = CriticNetwork(state_dim, action_dim)\n",
    "\n",
    "        self._actor.compile(optimizer=Adam(self._learning_rate))\n",
    "        self._critic.compile(optimizer=Adam(self._learning_rate))\n",
    "        self._target_actor.compile(optimizer=Adam(self._learning_rate))\n",
    "        self._target_critic.compile(optimizer=Adam(self._learning_rate))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Convert state into tensor and reshape to feed into network\n",
    "        numpy_state = np.array(state)\n",
    "        reshaped_state = numpy_state.reshape(1, self._state_dim[0])\n",
    "        tf_state = tf.convert_to_tensor(reshaped_state)\n",
    "        action = self._actor(tf_state)\n",
    "        clipped_action = tf.clip_by_value(action, self._min_action, self._max_action)\n",
    "\n",
    "        return clipped_action.numpy()[0] + self._noise.sample() # add noise\n",
    " \n",
    "    def store_transition(self, state, action, new_state, reward, terminal):\n",
    "        self._replay_memory.store_transition(state, action, new_state, reward, terminal)\n",
    "    \n",
    "    def store_last_transition(self, state, action, new_state, reward, terminal):\n",
    "        self._replay_memory.store_last_transition(state, action, new_state, reward, terminal)\n",
    "\n",
    "    def update_target_actor_weights(self):\n",
    "        actor_weights = self._actor.weights\n",
    "        target_actor_weights = self._target_actor.weights\n",
    "        updated_weights = []\n",
    "        for i, weight in enumerate(actor_weights):\n",
    "            updated_weights.append((weight*self._beta) + (1-self._beta)*target_actor_weights[i])\n",
    "        self._target_actor.set_weights(updated_weights)\n",
    "\n",
    "    def update_target_critic_weights(self):\n",
    "        critic_weights = self._critic.weights\n",
    "        target_critic_weights = self._target_critic.weights\n",
    "        updated_weights = []\n",
    "        for i, weight in enumerate(critic_weights):\n",
    "            updated_weights.append((weight*self._beta) + (1-self._beta)*target_critic_weights[i])\n",
    "        self._target_critic.set_weights(updated_weights)\n",
    "\n",
    "    def gradient_descent_critic(self, state, new_state, reward):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Convert to tensors and reshape to feed to network\n",
    "            action = self.get_action(state)\n",
    "            state_action = np.concatenate((state, action))\n",
    "            new_state_action = np.concatenate((new_state, action))\n",
    "            tf_state_action = tf.convert_to_tensor(state_action.reshape(1, self._state_dim_net))\n",
    "            tf_new_state_action = tf.convert_to_tensor(new_state_action.reshape(1, self._state_dim_net))\n",
    "\n",
    "            # Feed to network and get target\n",
    "            target_critic_value = self._target_critic(tf_new_state_action)\n",
    "            critic_value = self._critic(tf_state_action)\n",
    "            target = reward + self._gamma * target_critic_value\n",
    "            critic_loss = tf.keras.losses.MSE(target, critic_value)\n",
    "\n",
    "        # Perform gradient descent for critic with respect to critic weights\n",
    "        critic_gradients = tape.gradient(critic_loss, self._critic.trainable_variables)\n",
    "        self._critic.optimizer.apply_gradients(zip(critic_gradients, self._critic.trainable_variables))\n",
    "\n",
    "    def gradient_ascent_actor(self, state):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Convert to tensors and reshape to feed to network\n",
    "            action = self.get_action(state)\n",
    "            state_action = np.concatenate((state, action))\n",
    "            tf_state_action = tf.convert_to_tensor(state_action.reshape(1, self._state_dim_net))\n",
    "            critic_value = self._critic(tf_state_action)\n",
    "            actor_loss = - tf.reduce_mean(critic_value)\n",
    "        \n",
    "        # Perform gradient ascent for actor with respect to actor weights\n",
    "        actor_gradients = tape.gradient(actor_loss, self._actor.trainable_variables)\n",
    "        self._actor.optimizer.apply_gradients(zip(actor_gradients, self._actor.trainable_variables))\n",
    "\n",
    "    def learn(self):\n",
    "        samples = self._replay_memory.sample_transition(self._batch_size)\n",
    "        for transition in samples:\n",
    "            state, action, new_state, reward, terminal = transition\n",
    "\n",
    "            self.gradient_ascent_actor(state)\n",
    "            self.update_target_actor_weights()\n",
    "            self.gradient_descent_critic(state, new_state, reward)\n",
    "            self.update_target_critic_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['actor_network/dense/kernel:0', 'actor_network/dense/bias:0', 'actor_network/dense_1/kernel:0', 'actor_network/dense_1/bias:0', 'actor_network/dense_2/kernel:0', 'actor_network/dense_2/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'actor_network/dense/kernel:0' shape=(8, 256) dtype=float32, numpy=\narray([[-0.1270363 , -0.0531676 , -0.13230993, ..., -0.01138888,\n        -0.02405429, -0.10280645],\n       [-0.10561301,  0.03511579,  0.1497595 , ..., -0.05364694,\n        -0.12176427,  0.09937266],\n       [-0.02251211, -0.1274388 , -0.05422207, ...,  0.05141737,\n        -0.09575436, -0.10565147],\n       ...,\n       [-0.09025693, -0.07140909, -0.14514272, ...,  0.08167745,\n        -0.11591394,  0.11755019],\n       [-0.11041129, -0.02023545, -0.12238231, ...,  0.12690616,\n         0.01047863, -0.01740056],\n       [ 0.11758155, -0.09517828, -0.10438947, ..., -0.0165707 ,\n         0.11047366,  0.05663836]], dtype=float32)>), (None, <tf.Variable 'actor_network/dense/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'actor_network/dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\narray([[-0.0300518 ,  0.015643  ,  0.07542905, ...,  0.04001945,\n        -0.10457742,  0.03369015],\n       [-0.06714979, -0.02667472,  0.09278625, ...,  0.02379847,\n         0.04356837, -0.03836837],\n       [-0.02699602,  0.10133684, -0.00965819, ..., -0.01510745,\n        -0.04964533, -0.03063947],\n       ...,\n       [-0.12273344, -0.08878615,  0.02668807, ...,  0.0195525 ,\n        -0.00891274, -0.00022659],\n       [ 0.03654724,  0.01671699,  0.10452443, ...,  0.10211813,\n         0.05836168, -0.08880129],\n       [ 0.06880128, -0.03116229,  0.11223871, ...,  0.10326082,\n         0.06823215,  0.09043971]], dtype=float32)>), (None, <tf.Variable 'actor_network/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor_network/dense_2/kernel:0' shape=(128, 2) dtype=float32, numpy=\narray([[ 0.2104189 , -0.06193571],\n       [-0.15620187, -0.18151766],\n       [-0.12786575, -0.09091183],\n       [-0.16448392, -0.1799414 ],\n       [ 0.0561692 , -0.05567451],\n       [-0.18651181,  0.15266936],\n       [ 0.08773519, -0.17882366],\n       [ 0.20089383, -0.20972393],\n       [-0.20931663,  0.02897422],\n       [ 0.19764926,  0.11250891],\n       [-0.09808239,  0.17983334],\n       [-0.08618766, -0.07100728],\n       [-0.09277057,  0.04640959],\n       [ 0.09992529, -0.01525603],\n       [-0.08278923, -0.16503936],\n       [-0.17280018, -0.1611178 ],\n       [ 0.19953535, -0.11632996],\n       [ 0.1368403 ,  0.03042483],\n       [ 0.02570789, -0.06985962],\n       [-0.16039155, -0.09970168],\n       [-0.04587464, -0.06774135],\n       [-0.14311743, -0.09364624],\n       [ 0.2003598 ,  0.17305444],\n       [-0.04770203, -0.10845855],\n       [ 0.15506862,  0.13613014],\n       [-0.12207855,  0.19159381],\n       [-0.04918599, -0.14556864],\n       [-0.13570726,  0.09617977],\n       [ 0.09478866, -0.11523057],\n       [ 0.04827176, -0.1854518 ],\n       [ 0.11848046,  0.17097731],\n       [-0.02855657,  0.09161149],\n       [ 0.16683234, -0.06785235],\n       [-0.00742292,  0.11613257],\n       [ 0.03517653, -0.13065666],\n       [ 0.01251589,  0.12283538],\n       [-0.07370792, -0.18396282],\n       [-0.10769331, -0.02715032],\n       [ 0.20861937,  0.00615644],\n       [-0.11255343, -0.18039225],\n       [ 0.13437362, -0.06259984],\n       [-0.01841091,  0.12524368],\n       [-0.04200795,  0.04671918],\n       [-0.14494078, -0.07877815],\n       [ 0.05666582,  0.09269927],\n       [-0.15735987, -0.01129131],\n       [-0.09721144,  0.14731117],\n       [-0.06804165,  0.12763347],\n       [-0.19588374,  0.02132116],\n       [ 0.04019378, -0.20885831],\n       [-0.15812448,  0.00179826],\n       [-0.16353983, -0.01286726],\n       [ 0.09075265, -0.04199381],\n       [ 0.03707512, -0.20680304],\n       [-0.1985655 ,  0.2025655 ],\n       [-0.16397832,  0.01340482],\n       [ 0.09977446,  0.17556538],\n       [-0.01030603, -0.1251356 ],\n       [ 0.19725369,  0.1268874 ],\n       [ 0.13151218, -0.0802304 ],\n       [-0.06090716,  0.11034413],\n       [-0.01832598,  0.18641193],\n       [-0.12308574, -0.12609327],\n       [-0.0125203 , -0.17612635],\n       [ 0.14178677, -0.03896181],\n       [-0.10895292,  0.09257604],\n       [ 0.14794354,  0.14491592],\n       [-0.21475568,  0.19028364],\n       [ 0.1903279 ,  0.03049327],\n       [-0.01174815,  0.05858456],\n       [ 0.02351841,  0.1502295 ],\n       [-0.1922032 , -0.0507661 ],\n       [ 0.02278566, -0.09402675],\n       [ 0.11820872,  0.15498011],\n       [-0.06845722, -0.03530943],\n       [ 0.14011405, -0.04976505],\n       [-0.01579979, -0.14832896],\n       [ 0.16611074, -0.17413273],\n       [-0.09930534,  0.09429161],\n       [ 0.07011567, -0.16412635],\n       [ 0.01957265,  0.14947893],\n       [-0.10209916, -0.03394061],\n       [ 0.11449535,  0.04774834],\n       [ 0.02869733, -0.10418516],\n       [ 0.1642098 ,  0.20109113],\n       [ 0.09517758,  0.20614685],\n       [-0.07989956, -0.07451977],\n       [-0.00726752,  0.14963688],\n       [ 0.19123708, -0.01501581],\n       [-0.10373995,  0.13790144],\n       [-0.05393793, -0.05884701],\n       [ 0.08632334, -0.14812368],\n       [ 0.05047779,  0.07870926],\n       [ 0.2108656 ,  0.08646174],\n       [ 0.08188479, -0.11224324],\n       [-0.10149655,  0.17598431],\n       [ 0.02309734,  0.01309966],\n       [-0.15668876, -0.01092345],\n       [ 0.07611407, -0.00452349],\n       [ 0.02974622, -0.15777172],\n       [ 0.00780575,  0.05215122],\n       [-0.00980268,  0.01483588],\n       [ 0.06411053,  0.07712366],\n       [ 0.18163137,  0.07664974],\n       [ 0.07115258,  0.00402834],\n       [-0.15086919,  0.06656601],\n       [-0.08345233, -0.16285628],\n       [-0.10763871, -0.12026795],\n       [-0.10686672, -0.08370392],\n       [-0.18356013, -0.09586086],\n       [-0.07052058,  0.11541532],\n       [-0.0546083 , -0.11053953],\n       [ 0.02560627, -0.15222377],\n       [-0.11686634, -0.10999752],\n       [ 0.16588588, -0.10976616],\n       [-0.04989715, -0.0440852 ],\n       [ 0.02371772, -0.16428041],\n       [-0.0271205 ,  0.21350713],\n       [-0.13895252, -0.1395449 ],\n       [-0.14183046,  0.16596259],\n       [ 0.11900802, -0.14726123],\n       [-0.10930589,  0.17486344],\n       [-0.09747308,  0.04053976],\n       [ 0.13350327, -0.16310444],\n       [-0.19545016,  0.09368823],\n       [ 0.03905495, -0.06966729],\n       [ 0.11901627,  0.04875718],\n       [-0.16432631, -0.19607782]], dtype=float32)>), (None, <tf.Variable 'actor_network/dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[0;32m     22\u001b[0m     episode_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 23\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m episode_score_history\u001b[38;5;241m.\u001b[39mappend(episode_score)\n\u001b[0;32m     26\u001b[0m avg_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(episode_score_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:])\n",
      "Cell \u001b[1;32mIn[5], line 102\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transition \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[0;32m    100\u001b[0m     state, action, new_state, reward, terminal \u001b[38;5;241m=\u001b[39m transition\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_ascent_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target_actor_weights()\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_descent_critic(state, new_state, reward)\n",
      "Cell \u001b[1;32mIn[5], line 95\u001b[0m, in \u001b[0;36mAgent.gradient_ascent_actor\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Perform gradient ascent for actor with respect to actor weights\u001b[39;00m\n\u001b[0;32m     94\u001b[0m actor_gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(actor_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactor_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:633\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m                     grads_and_vars,\n\u001b[0;32m    594\u001b[0m                     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    595\u001b[0m                     experimental_aggregate_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m  This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;124;03m    RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m   var_list \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m (_, v) \u001b[38;5;129;01min\u001b[39;00m grads_and_vars]\n\u001b[0;32m    636\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name):\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizer_v2\\utils.py:73\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[0;32m     72\u001b[0m   variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n\u001b[0;32m     76\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m     77\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m when minimizing the loss. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using `model.compile()`, did you forget to provide a `loss`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument?\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     80\u001b[0m       ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vars_with_empty_grads]))\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['actor_network/dense/kernel:0', 'actor_network/dense/bias:0', 'actor_network/dense_1/kernel:0', 'actor_network/dense_1/bias:0', 'actor_network/dense_2/kernel:0', 'actor_network/dense_2/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'actor_network/dense/kernel:0' shape=(8, 256) dtype=float32, numpy=\narray([[-0.1270363 , -0.0531676 , -0.13230993, ..., -0.01138888,\n        -0.02405429, -0.10280645],\n       [-0.10561301,  0.03511579,  0.1497595 , ..., -0.05364694,\n        -0.12176427,  0.09937266],\n       [-0.02251211, -0.1274388 , -0.05422207, ...,  0.05141737,\n        -0.09575436, -0.10565147],\n       ...,\n       [-0.09025693, -0.07140909, -0.14514272, ...,  0.08167745,\n        -0.11591394,  0.11755019],\n       [-0.11041129, -0.02023545, -0.12238231, ...,  0.12690616,\n         0.01047863, -0.01740056],\n       [ 0.11758155, -0.09517828, -0.10438947, ..., -0.0165707 ,\n         0.11047366,  0.05663836]], dtype=float32)>), (None, <tf.Variable 'actor_network/dense/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'actor_network/dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\narray([[-0.0300518 ,  0.015643  ,  0.07542905, ...,  0.04001945,\n        -0.10457742,  0.03369015],\n       [-0.06714979, -0.02667472,  0.09278625, ...,  0.02379847,\n         0.04356837, -0.03836837],\n       [-0.02699602,  0.10133684, -0.00965819, ..., -0.01510745,\n        -0.04964533, -0.03063947],\n       ...,\n       [-0.12273344, -0.08878615,  0.02668807, ...,  0.0195525 ,\n        -0.00891274, -0.00022659],\n       [ 0.03654724,  0.01671699,  0.10452443, ...,  0.10211813,\n         0.05836168, -0.08880129],\n       [ 0.06880128, -0.03116229,  0.11223871, ...,  0.10326082,\n         0.06823215,  0.09043971]], dtype=float32)>), (None, <tf.Variable 'actor_network/dense_1/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'actor_network/dense_2/kernel:0' shape=(128, 2) dtype=float32, numpy=\narray([[ 0.2104189 , -0.06193571],\n       [-0.15620187, -0.18151766],\n       [-0.12786575, -0.09091183],\n       [-0.16448392, -0.1799414 ],\n       [ 0.0561692 , -0.05567451],\n       [-0.18651181,  0.15266936],\n       [ 0.08773519, -0.17882366],\n       [ 0.20089383, -0.20972393],\n       [-0.20931663,  0.02897422],\n       [ 0.19764926,  0.11250891],\n       [-0.09808239,  0.17983334],\n       [-0.08618766, -0.07100728],\n       [-0.09277057,  0.04640959],\n       [ 0.09992529, -0.01525603],\n       [-0.08278923, -0.16503936],\n       [-0.17280018, -0.1611178 ],\n       [ 0.19953535, -0.11632996],\n       [ 0.1368403 ,  0.03042483],\n       [ 0.02570789, -0.06985962],\n       [-0.16039155, -0.09970168],\n       [-0.04587464, -0.06774135],\n       [-0.14311743, -0.09364624],\n       [ 0.2003598 ,  0.17305444],\n       [-0.04770203, -0.10845855],\n       [ 0.15506862,  0.13613014],\n       [-0.12207855,  0.19159381],\n       [-0.04918599, -0.14556864],\n       [-0.13570726,  0.09617977],\n       [ 0.09478866, -0.11523057],\n       [ 0.04827176, -0.1854518 ],\n       [ 0.11848046,  0.17097731],\n       [-0.02855657,  0.09161149],\n       [ 0.16683234, -0.06785235],\n       [-0.00742292,  0.11613257],\n       [ 0.03517653, -0.13065666],\n       [ 0.01251589,  0.12283538],\n       [-0.07370792, -0.18396282],\n       [-0.10769331, -0.02715032],\n       [ 0.20861937,  0.00615644],\n       [-0.11255343, -0.18039225],\n       [ 0.13437362, -0.06259984],\n       [-0.01841091,  0.12524368],\n       [-0.04200795,  0.04671918],\n       [-0.14494078, -0.07877815],\n       [ 0.05666582,  0.09269927],\n       [-0.15735987, -0.01129131],\n       [-0.09721144,  0.14731117],\n       [-0.06804165,  0.12763347],\n       [-0.19588374,  0.02132116],\n       [ 0.04019378, -0.20885831],\n       [-0.15812448,  0.00179826],\n       [-0.16353983, -0.01286726],\n       [ 0.09075265, -0.04199381],\n       [ 0.03707512, -0.20680304],\n       [-0.1985655 ,  0.2025655 ],\n       [-0.16397832,  0.01340482],\n       [ 0.09977446,  0.17556538],\n       [-0.01030603, -0.1251356 ],\n       [ 0.19725369,  0.1268874 ],\n       [ 0.13151218, -0.0802304 ],\n       [-0.06090716,  0.11034413],\n       [-0.01832598,  0.18641193],\n       [-0.12308574, -0.12609327],\n       [-0.0125203 , -0.17612635],\n       [ 0.14178677, -0.03896181],\n       [-0.10895292,  0.09257604],\n       [ 0.14794354,  0.14491592],\n       [-0.21475568,  0.19028364],\n       [ 0.1903279 ,  0.03049327],\n       [-0.01174815,  0.05858456],\n       [ 0.02351841,  0.1502295 ],\n       [-0.1922032 , -0.0507661 ],\n       [ 0.02278566, -0.09402675],\n       [ 0.11820872,  0.15498011],\n       [-0.06845722, -0.03530943],\n       [ 0.14011405, -0.04976505],\n       [-0.01579979, -0.14832896],\n       [ 0.16611074, -0.17413273],\n       [-0.09930534,  0.09429161],\n       [ 0.07011567, -0.16412635],\n       [ 0.01957265,  0.14947893],\n       [-0.10209916, -0.03394061],\n       [ 0.11449535,  0.04774834],\n       [ 0.02869733, -0.10418516],\n       [ 0.1642098 ,  0.20109113],\n       [ 0.09517758,  0.20614685],\n       [-0.07989956, -0.07451977],\n       [-0.00726752,  0.14963688],\n       [ 0.19123708, -0.01501581],\n       [-0.10373995,  0.13790144],\n       [-0.05393793, -0.05884701],\n       [ 0.08632334, -0.14812368],\n       [ 0.05047779,  0.07870926],\n       [ 0.2108656 ,  0.08646174],\n       [ 0.08188479, -0.11224324],\n       [-0.10149655,  0.17598431],\n       [ 0.02309734,  0.01309966],\n       [-0.15668876, -0.01092345],\n       [ 0.07611407, -0.00452349],\n       [ 0.02974622, -0.15777172],\n       [ 0.00780575,  0.05215122],\n       [-0.00980268,  0.01483588],\n       [ 0.06411053,  0.07712366],\n       [ 0.18163137,  0.07664974],\n       [ 0.07115258,  0.00402834],\n       [-0.15086919,  0.06656601],\n       [-0.08345233, -0.16285628],\n       [-0.10763871, -0.12026795],\n       [-0.10686672, -0.08370392],\n       [-0.18356013, -0.09586086],\n       [-0.07052058,  0.11541532],\n       [-0.0546083 , -0.11053953],\n       [ 0.02560627, -0.15222377],\n       [-0.11686634, -0.10999752],\n       [ 0.16588588, -0.10976616],\n       [-0.04989715, -0.0440852 ],\n       [ 0.02371772, -0.16428041],\n       [-0.0271205 ,  0.21350713],\n       [-0.13895252, -0.1395449 ],\n       [-0.14183046,  0.16596259],\n       [ 0.11900802, -0.14726123],\n       [-0.10930589,  0.17486344],\n       [-0.09747308,  0.04053976],\n       [ 0.13350327, -0.16310444],\n       [-0.19545016,  0.09368823],\n       [ 0.03905495, -0.06966729],\n       [ 0.11901627,  0.04875718],\n       [-0.16432631, -0.19607782]], dtype=float32)>), (None, <tf.Variable 'actor_network/dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", continuous= True, render_mode=\"human\")\n",
    "n_games = 1000\n",
    "best_score = float('-inf')\n",
    "episode_score_history = []\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape\n",
    "agent = Agent(state_dim, action_dim)\n",
    "\n",
    "for i in range(n_games):\n",
    "    current_state = env.reset()[0]\n",
    "    episode_score = 0\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        action = agent.get_action(current_state)\n",
    "        new_state, reward, terminal, truncated, info = env.step(action)\n",
    "        terminal = terminal or truncated\n",
    "        agent.store_transition(current_state, action, new_state, reward, terminal)\n",
    "        agent.store_last_transition(current_state, action, new_state, reward, terminal)\n",
    "        current_state = new_state\n",
    "        episode_score += reward\n",
    "        agent.learn()\n",
    "\n",
    "    episode_score_history.append(episode_score)\n",
    "    avg_score = np.mean(episode_score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_avg_score = avg_score\n",
    "    print(f\"Episode: {i}, score: {episode_score}, avg_score: {avg_score}, best_score: {max(episode_score_history)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
