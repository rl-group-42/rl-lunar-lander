{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44d4b4b-3691-493f-bd6c-13fd57fe7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class DQN(Module):\n",
    "    def __init__(self,\n",
    "                 n_states: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden_neurons: int,\n",
    "                 duelling: bool = False\n",
    "                ):\n",
    "        super(DQN, self).__init__()\n",
    "        self.duelling = duelling\n",
    "        self.l1 = Linear(n_states, n_hidden_neurons)\n",
    "        self.l2 = Linear(n_hidden_neurons, n_hidden_neurons)\n",
    "        self.l3 = Linear(n_hidden_neurons, n_actions)\n",
    "        if duelling:\n",
    "            self.l4 = Linear(n_hidden_neurons, 1) # state value estimation\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        x = self.l1(state).relu()\n",
    "        x = self.l2(x).relu()\n",
    "        if self.duelling:\n",
    "            return self.l4(x) + (self.l3(x) - self.l3(x).mean(dim=1, keepdim=True)[0])\n",
    "        else:\n",
    "            return self.l3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6ff3da-f558-4b19-8e2b-284cf19d2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "from numpy.typing import NDArray\n",
    "from typing import List, Tuple\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,\n",
    "                 max_buffer_size: int\n",
    "                ):\n",
    "        self.buffer = deque(maxlen=max_buffer_size)\n",
    "\n",
    "\n",
    "    def add(self,\n",
    "            state: NDArray[float32],\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            next: NDArray[float32],\n",
    "            terminal: bool\n",
    "           ):\n",
    "        self.buffer.append((state, action, reward, next, terminal))\n",
    "\n",
    "\n",
    "    def sample(self, \n",
    "               batches: int\n",
    "              ) -> Tuple[NDArray[float32], List[int], List[float], NDArray[float32], List[bool]]:\n",
    "        samples = random.sample(self.buffer, batches)\n",
    "        return zip(*samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc043ddd-da8e-4fbc-a796-3e06a5b1bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PrioritisedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self,\n",
    "                 max_buffer_size: int,\n",
    "                 bias_factor_start: float,\n",
    "                 bias_factor_end: float,\n",
    "                 bias_increment: float,\n",
    "                 priority_scale: float,\n",
    "                 td_error_clamp: int\n",
    "                ):\n",
    "        super(PrioritisedReplayBuffer, self).__init__(max_buffer_size)\n",
    "        self.priorities = deque(maxlen=max_buffer_size)\n",
    "        self.min_priority = 0.01\n",
    "        self.bias_factor = bias_factor_start\n",
    "        self.bias_factor_end = bias_factor_end\n",
    "        self.bias_increment = bias_increment\n",
    "        self.priority_scale = priority_scale\n",
    "        self.td_error_clamp = td_error_clamp\n",
    "\n",
    "    def add(self,\n",
    "            state: NDArray[float32],\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            next: NDArray[float32],\n",
    "            terminal: bool,\n",
    "            error: Tensor\n",
    "           ):\n",
    "        super(PrioritisedReplayBuffer, self).add(state, action, reward, next, terminal)\n",
    "        error = np.clip(error, a_min=-self.td_error_clamp, a_max=self.td_error_clamp)\n",
    "        self.priorities.append((np.abs(error) + self.min_priority)**self.priority_scale)\n",
    "\n",
    "    def sample(self,\n",
    "               batches: int\n",
    "              ) -> Tuple[NDArray[float32], List[int], List[float], NDArray[float32], List[bool], List[int], List[float]]:\n",
    "        probas = np.array(self.priorities)/np.sum(self.priorities)\n",
    "        indices = np.random.choice(len(self.buffer), batches, p=probas)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        states, actions, rewards, nexts, terminals = zip(*samples)\n",
    "        weights = (1/len(self.buffer) * 1/probas[indices]) ** self.bias_factor\n",
    "        weights /= weights.sum()\n",
    "        return states, actions, rewards, nexts, terminals, indices, weights\n",
    "\n",
    "    def update_priorities(self, idx, error):\n",
    "        self.priorities[idx] = (np.abs(error) + self.min_priority) ** self.priority_scale\n",
    "\n",
    "    def update_bias_factor(self):\n",
    "        self.bias_factor = min(self.bias_factor + self.bias_increment, self.bias_factor_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dcf24b-68b3-45a8-a1d8-d6b9e397827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from torch.backends import mps\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import MSELoss\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "def _get_torch_device() -> str:\n",
    "    if cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 n_states: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden_neurons: int,\n",
    "                 learning_rate: float,\n",
    "                 discount_factor: float,\n",
    "                 max_buffer_size: int,\n",
    "                 batch_size: int,\n",
    "                 modifications: List[str] = None,\n",
    "                 per_params: Dict[str, float] = None\n",
    "                ):\n",
    "        self.device = torch.device(_get_torch_device())\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.qnet = DQN(n_states, n_actions, n_hidden_neurons, 'duelling' in modifications).to(self.device)\n",
    "        self.modifications = modifications\n",
    "\n",
    "        # self.optimiser = SGD(self.qnet.parameters(), lr=learning_rate)\n",
    "        self.optimiser = Adam(self.qnet.parameters(), lr=learning_rate)\n",
    "        if 'per' in modifications:\n",
    "            self.replay_buffer = PrioritisedReplayBuffer(max_buffer_size, **per_params)\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(max_buffer_size)\n",
    "        self.target_qnet = DQN(n_states, n_actions, n_hidden_neurons, 'duelling' in modifications).to(self.device)\n",
    "        self.target_qnet.load_state_dict(self.qnet.state_dict())\n",
    "        self.target_qnet.eval()\n",
    "\n",
    "    \n",
    "    def get_td_error(self,\n",
    "                     state: float32,\n",
    "                     action: int,\n",
    "                     reward: float,\n",
    "                     next: float32,\n",
    "                     terminal: bool\n",
    "                    ):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        action_tensor = torch.tensor([action], device=self.device)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        next_tensor = torch.from_numpy(next).float().unsqueeze(0).to(self.device)\n",
    "        terminal_tensor = torch.tensor([terminal], device=self.device)\n",
    "        curr_q = self.qnet(state_tensor).gather(1, action_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "        next_q = self.target_qnet(next_tensor).max(1)[0]\n",
    "        expected_q = reward_tensor + self.discount_factor * next_q * (1 - terminal_tensor.float())\n",
    "        return MSELoss()(curr_q, expected_q).item()\n",
    "\n",
    "\n",
    "    def _step_no_per(self,\n",
    "             state: NDArray[float32],\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next: NDArray[float32],\n",
    "             terminal: bool\n",
    "            ):\n",
    "        self.replay_buffer.add(state, action, reward, next, terminal)\n",
    "        if len(self.replay_buffer.buffer) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "\n",
    "    def _step_per(self,\n",
    "             state: NDArray[float32],\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next: NDArray[float32],\n",
    "             terminal: bool\n",
    "            ):\n",
    "        error = self.get_td_error(state, action, reward, next, terminal)\n",
    "        self.replay_buffer.add(state, action, reward, next, terminal, error)\n",
    "        if len(self.replay_buffer.buffer) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "\n",
    "    def step(self,\n",
    "             state: NDArray[float32],\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next: NDArray[float32],\n",
    "             terminal: bool\n",
    "            ):\n",
    "        if 'per' in self.modifications:\n",
    "            self._step_per(state, action, reward, next, terminal)\n",
    "        else:\n",
    "            self._step_no_per(state, action, reward, next, terminal)\n",
    "\n",
    "\n",
    "    def act(self,\n",
    "            state: NDArray[float32],\n",
    "            exploration_chance: float\n",
    "           ) -> int:\n",
    "        if random.random() > exploration_chance:\n",
    "            self.qnet.eval()\n",
    "            state_tensor_batched = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            state = state_tensor_batched.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnet(state)\n",
    "            chosen_action = np.argmax(action_values.cpu().detach().numpy())\n",
    "            self.qnet.train()\n",
    "            return chosen_action\n",
    "        else:\n",
    "            return random.choice(np.arange(self.n_actions))\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        if 'per' in self.modifications:\n",
    "            states, actions, rewards, nexts, terminals, indices, weights = self.replay_buffer.sample(self.batch_size)\n",
    "        else:\n",
    "            states, actions, rewards, nexts, terminals = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack(states)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "        nexts = torch.from_numpy(np.stack(nexts)).float().to(self.device)\n",
    "        terminals = torch.from_numpy(np.array(terminals)).float().to(self.device)\n",
    "\n",
    "        q_values = self.qnet(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        if 'double' in self.modifications:\n",
    "            next_q_values = self.target_qnet(nexts).gather(1, self.qnet(nexts).max(1)[1].unsqueeze(-1)).detach().squeeze(-1)\n",
    "        else:\n",
    "            next_q_values = self.target_qnet(nexts).max(1)[0].detach()\n",
    "\n",
    "        expected_q_values = rewards + self.discount_factor * next_q_values * (1 - terminals)\n",
    "\n",
    "        loss = MSELoss()(q_values, expected_q_values)\n",
    "        if 'per' in self.modifications:\n",
    "            loss = (loss * torch.from_numpy(weights).float().to(self.device)).mean()\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimiser.step()\n",
    "\n",
    "        if 'per' in self.modifications:\n",
    "            for i, (state,\n",
    "                      action,\n",
    "                      reward,\n",
    "                      next,\n",
    "                      terminal\n",
    "                     ) in enumerate(zip(states.cpu().numpy(),\n",
    "                                        actions.cpu().numpy(),\n",
    "                                        rewards.cpu().numpy(),\n",
    "                                        nexts.cpu().numpy(),\n",
    "                                        terminals.cpu().numpy())\n",
    "                                   ):\n",
    "                error = self.get_td_error(state, action, reward, next, terminal)\n",
    "                self.replay_buffer.update_priorities(i, error)\n",
    "            self.replay_buffer.update_bias_factor()\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_qnet.load_state_dict(self.qnet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd30b391-5428-448e-8497-860d8c33407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'n_hidden_neurons': 128,\n",
    "'learning_rate': 5e-4,\n",
    "'discount_factor': 0.99,\n",
    "'max_buffer_size': 10_000,\n",
    "'batch_size': 32,\n",
    "'n_episodes': 10_000,\n",
    "'exploration_chance_start': 1.0,\n",
    "'exploration_chance_end': 1e-4,\n",
    "'exploration_chance_decay': 0.99,\n",
    "'target_update_freq': 20,\n",
    "'finish_check_freq': 100,\n",
    "'finish_score': 200,\n",
    "'per_params': {\n",
    "    'bias_factor_start': 0.5,\n",
    "    'bias_factor_end': 1.0,\n",
    "    'bias_increment': 1e-2,\n",
    "    'priority_scale': 0.7,\n",
    "    'td_error_clamp': 50 # 1/4 of target end score\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73df0e5-4d62-47d0-af7f-851f58459f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "def train(params: Dict[str, Union[int, float]], modifications: List[str] = [], save_data=False) -> DQNAgent:\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    if save_data:\n",
    "        data_fname = f'DQN_{\"_\".join(modifications)}_{time.time()}.txt'\n",
    "\n",
    "        with open(data_fname, 'w') as f:\n",
    "            f.write(json.dumps(params))\n",
    "            f.write('\\n---\\n')\n",
    "\n",
    "    agent = DQNAgent(n_states,\n",
    "                     n_actions,\n",
    "                     params['n_hidden_neurons'],\n",
    "                     params['learning_rate'],\n",
    "                     params['discount_factor'],\n",
    "                     params['max_buffer_size'],\n",
    "                     params['batch_size'],\n",
    "                     modifications=modifications,\n",
    "                     per_params=params['per_params'] if 'per' in modifications else None\n",
    "                    )\n",
    "\n",
    "    scores = []\n",
    "    latest_scores = deque(maxlen=params['finish_check_freq'])\n",
    "    \n",
    "    exploration_chance = params['exploration_chance_start']\n",
    "\n",
    "    for episode_n in range(1, params['n_episodes'] + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        steps = 0\n",
    "    \n",
    "        while True:\n",
    "            action = agent.act(state, exploration_chance)\n",
    "            next, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            terminal = terminated or truncated\n",
    "            agent.step(state, action, reward, next, terminal)\n",
    "            \n",
    "            state = next\n",
    "            score += reward\n",
    "            steps += 1\n",
    "    \n",
    "            if terminal:\n",
    "                break\n",
    "    \n",
    "        scores.append(score)\n",
    "        latest_scores.append(score)\n",
    "    \n",
    "        exploration_chance = max(params['exploration_chance_end'], params['exploration_chance_decay'] * exploration_chance)\n",
    "    \n",
    "        if episode_n % params['target_update_freq'] == 0:\n",
    "            agent.update_target_network()\n",
    "        if save_data:\n",
    "            with open(data_fname, 'a') as f:\n",
    "                f.write(f'\\n{score}@{episode_n}:{steps}')\n",
    "        if episode_n % params['finish_check_freq'] == 0:\n",
    "            print(f'Average score of {np.mean(latest_scores):0.3f} @ {episode_n}/{params[\"n_episodes\"]}')\n",
    "            if np.mean(latest_scores) >= params['finish_score']:\n",
    "                print(f'Average score was above {params[\"finish_score\"]} over last {params[\"finish_check_freq\"]} episodes. Ending training...')\n",
    "                break\n",
    "    env.close()\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "599d5aa4-41ed-427c-bfb7-75215c53efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_run_agent(agent):\n",
    "    env = gym.make('LunarLander-v2', render_mode='human')\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state, 0)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "    \n",
    "        terminal = terminated or truncated\n",
    "    \n",
    "        score += reward\n",
    "    \n",
    "        if terminal:\n",
    "            break\n",
    "    \n",
    "    print(f'Score achieved on test: {score}')\n",
    "    print(f'Steps taken until termination: {steps}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a6bd9-344f-4b72-aa30-a474df43b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0/24\n",
      "{'n_hidden_neurons': 64, 'learning_rate': 0.001, 'discount_factor': 0.99, 'max_buffer_size': 10000, 'batch_size': 32, 'n_episodes': 10000, 'exploration_chance_start': 1.0, 'exploration_chance_end': 1e-05, 'exploration_chance_decay': 0.995, 'target_update_freq': 10, 'finish_check_freq': 100, 'finish_score': 200, 'per_params': {'bias_factor_start': 0.5, 'bias_factor_end': 1.0, 'bias_increment': 0.01, 'priority_scale': 0.7, 'td_error_clamp': 50}}\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/git/rl-lunar-lander/.venv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score of -147.601 @ 100/10000\n",
      "Average score of -50.143 @ 200/10000\n",
      "Average score of 4.726 @ 300/10000\n",
      "Average score of 50.524 @ 400/10000\n",
      "Average score of 85.000 @ 500/10000\n",
      "Average score of 126.376 @ 600/10000\n",
      "Average score of 127.217 @ 700/10000\n",
      "Average score of 164.118 @ 800/10000\n",
      "Average score of 157.203 @ 900/10000\n",
      "Average score of 232.761 @ 1000/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1\n",
      "Average score of -125.274 @ 100/10000\n",
      "Average score of -58.371 @ 200/10000\n",
      "Average score of 16.200 @ 300/10000\n",
      "Average score of 28.196 @ 400/10000\n",
      "Average score of 62.706 @ 500/10000\n",
      "Average score of 31.682 @ 600/10000\n",
      "Average score of 109.152 @ 700/10000\n",
      "Average score of 107.662 @ 800/10000\n",
      "Average score of 77.923 @ 900/10000\n",
      "Average score of 199.211 @ 1000/10000\n",
      "Average score of 177.310 @ 1100/10000\n",
      "Average score of 123.506 @ 1200/10000\n",
      "Average score of 8.900 @ 1300/10000\n",
      "Average score of 86.091 @ 1400/10000\n",
      "Average score of 65.744 @ 1500/10000\n",
      "Average score of 165.888 @ 1600/10000\n",
      "Average score of 184.524 @ 1700/10000\n",
      "Average score of 212.909 @ 1800/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2\n",
      "Average score of -141.534 @ 100/10000\n",
      "Average score of -52.901 @ 200/10000\n",
      "Average score of 28.646 @ 300/10000\n",
      "Average score of 19.683 @ 400/10000\n",
      "Average score of 31.880 @ 500/10000\n",
      "Average score of 148.812 @ 600/10000\n",
      "Average score of 102.020 @ 700/10000\n",
      "Average score of 30.028 @ 800/10000\n",
      "Average score of 32.088 @ 900/10000\n",
      "Average score of 131.771 @ 1000/10000\n",
      "Average score of 171.954 @ 1100/10000\n",
      "Average score of 230.170 @ 1200/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1/24\n",
      "{'n_hidden_neurons': 64, 'learning_rate': 0.001, 'discount_factor': 0.99, 'max_buffer_size': 10000, 'batch_size': 32, 'n_episodes': 10000, 'exploration_chance_start': 1.0, 'exploration_chance_end': 1e-05, 'exploration_chance_decay': 0.995, 'target_update_freq': 20, 'finish_check_freq': 100, 'finish_score': 200, 'per_params': {'bias_factor_start': 0.5, 'bias_factor_end': 1.0, 'bias_increment': 0.01, 'priority_scale': 0.7, 'td_error_clamp': 50}}\n",
      "0\n",
      "Average score of -136.287 @ 100/10000\n",
      "Average score of -106.589 @ 200/10000\n",
      "Average score of -49.454 @ 300/10000\n",
      "Average score of -57.197 @ 400/10000\n",
      "Average score of -79.292 @ 500/10000\n",
      "Average score of -49.015 @ 600/10000\n",
      "Average score of -68.986 @ 700/10000\n",
      "Average score of -32.823 @ 800/10000\n",
      "Average score of -70.345 @ 900/10000\n",
      "Average score of -67.892 @ 1000/10000\n",
      "Average score of -17.722 @ 1100/10000\n",
      "Average score of 89.026 @ 1200/10000\n",
      "Average score of 227.062 @ 1300/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1\n",
      "Average score of -142.473 @ 100/10000\n",
      "Average score of -57.333 @ 200/10000\n",
      "Average score of -18.100 @ 300/10000\n",
      "Average score of -67.150 @ 400/10000\n",
      "Average score of -0.251 @ 500/10000\n",
      "Average score of 36.105 @ 600/10000\n",
      "Average score of 91.778 @ 700/10000\n",
      "Average score of 120.542 @ 800/10000\n",
      "Average score of 101.403 @ 900/10000\n",
      "Average score of 169.227 @ 1000/10000\n",
      "Average score of 189.302 @ 1100/10000\n",
      "Average score of 205.695 @ 1200/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2\n",
      "Average score of -151.224 @ 100/10000\n",
      "Average score of -73.441 @ 200/10000\n",
      "Average score of -70.455 @ 300/10000\n",
      "Average score of -76.027 @ 400/10000\n",
      "Average score of -50.999 @ 500/10000\n",
      "Average score of 49.686 @ 600/10000\n",
      "Average score of 65.476 @ 700/10000\n",
      "Average score of 23.053 @ 800/10000\n",
      "Average score of 101.580 @ 900/10000\n",
      "Average score of 190.325 @ 1000/10000\n",
      "Average score of 195.944 @ 1100/10000\n",
      "Average score of 217.889 @ 1200/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2/24\n",
      "{'n_hidden_neurons': 64, 'learning_rate': 0.001, 'discount_factor': 0.99, 'max_buffer_size': 10000, 'batch_size': 64, 'n_episodes': 10000, 'exploration_chance_start': 1.0, 'exploration_chance_end': 1e-05, 'exploration_chance_decay': 0.995, 'target_update_freq': 10, 'finish_check_freq': 100, 'finish_score': 200, 'per_params': {'bias_factor_start': 0.5, 'bias_factor_end': 1.0, 'bias_increment': 0.01, 'priority_scale': 0.7, 'td_error_clamp': 50}}\n",
      "0\n",
      "Average score of -132.010 @ 100/10000\n",
      "Average score of -62.164 @ 200/10000\n",
      "Average score of -54.624 @ 300/10000\n",
      "Average score of 20.077 @ 400/10000\n",
      "Average score of 114.818 @ 500/10000\n",
      "Average score of 66.768 @ 600/10000\n",
      "Average score of 109.030 @ 700/10000\n",
      "Average score of 169.771 @ 800/10000\n",
      "Average score of 190.443 @ 900/10000\n",
      "Average score of 125.545 @ 1000/10000\n",
      "Average score of 97.265 @ 1100/10000\n",
      "Average score of 192.036 @ 1200/10000\n",
      "Average score of 206.979 @ 1300/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1\n",
      "Average score of -132.192 @ 100/10000\n",
      "Average score of -70.253 @ 200/10000\n",
      "Average score of -22.256 @ 300/10000\n",
      "Average score of 39.935 @ 400/10000\n",
      "Average score of 49.649 @ 500/10000\n",
      "Average score of 113.946 @ 600/10000\n",
      "Average score of 55.080 @ 700/10000\n",
      "Average score of 50.607 @ 800/10000\n",
      "Average score of 131.643 @ 900/10000\n",
      "Average score of 159.104 @ 1000/10000\n",
      "Average score of 221.206 @ 1100/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2\n",
      "Average score of -130.161 @ 100/10000\n",
      "Average score of -22.411 @ 200/10000\n",
      "Average score of -17.999 @ 300/10000\n",
      "Average score of 64.820 @ 400/10000\n",
      "Average score of 95.988 @ 500/10000\n",
      "Average score of 57.112 @ 600/10000\n",
      "Average score of 86.195 @ 700/10000\n",
      "Average score of 122.694 @ 800/10000\n",
      "Average score of 167.233 @ 900/10000\n",
      "Average score of 174.403 @ 1000/10000\n",
      "Average score of 166.222 @ 1100/10000\n",
      "Average score of 172.945 @ 1200/10000\n",
      "Average score of 192.182 @ 1300/10000\n",
      "Average score of 194.479 @ 1400/10000\n",
      "Average score of 155.060 @ 1500/10000\n",
      "Average score of 123.913 @ 1600/10000\n",
      "Average score of 233.932 @ 1700/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "3/24\n",
      "{'n_hidden_neurons': 64, 'learning_rate': 0.001, 'discount_factor': 0.99, 'max_buffer_size': 10000, 'batch_size': 64, 'n_episodes': 10000, 'exploration_chance_start': 1.0, 'exploration_chance_end': 1e-05, 'exploration_chance_decay': 0.995, 'target_update_freq': 20, 'finish_check_freq': 100, 'finish_score': 200, 'per_params': {'bias_factor_start': 0.5, 'bias_factor_end': 1.0, 'bias_increment': 0.01, 'priority_scale': 0.7, 'td_error_clamp': 50}}\n",
      "0\n",
      "Average score of -134.779 @ 100/10000\n",
      "Average score of -81.169 @ 200/10000\n",
      "Average score of -30.648 @ 300/10000\n",
      "Average score of 3.897 @ 400/10000\n",
      "Average score of 24.080 @ 500/10000\n",
      "Average score of 2.023 @ 600/10000\n",
      "Average score of 49.820 @ 700/10000\n",
      "Average score of 80.549 @ 800/10000\n",
      "Average score of 121.809 @ 900/10000\n",
      "Average score of 131.453 @ 1000/10000\n",
      "Average score of 99.110 @ 1100/10000\n",
      "Average score of 51.187 @ 1200/10000\n",
      "Average score of 156.897 @ 1300/10000\n",
      "Average score of 69.602 @ 1400/10000\n",
      "Average score of 22.003 @ 1500/10000\n",
      "Average score of -47.796 @ 1600/10000\n",
      "Average score of -156.691 @ 1700/10000\n",
      "Average score of -52.348 @ 1800/10000\n",
      "Average score of 168.066 @ 1900/10000\n",
      "Average score of 216.403 @ 2000/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1\n",
      "Average score of -141.416 @ 100/10000\n",
      "Average score of -109.359 @ 200/10000\n",
      "Average score of -39.034 @ 300/10000\n",
      "Average score of -68.724 @ 400/10000\n",
      "Average score of -8.729 @ 500/10000\n",
      "Average score of 52.474 @ 600/10000\n",
      "Average score of 26.064 @ 700/10000\n",
      "Average score of 85.228 @ 800/10000\n",
      "Average score of 111.552 @ 900/10000\n",
      "Average score of 125.327 @ 1000/10000\n",
      "Average score of 150.434 @ 1100/10000\n",
      "Average score of 132.668 @ 1200/10000\n",
      "Average score of 199.591 @ 1300/10000\n",
      "Average score of 190.488 @ 1400/10000\n",
      "Average score of 203.713 @ 1500/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2\n",
      "Average score of -155.821 @ 100/10000\n",
      "Average score of -53.846 @ 200/10000\n",
      "Average score of -4.229 @ 300/10000\n",
      "Average score of -31.775 @ 400/10000\n",
      "Average score of -17.629 @ 500/10000\n",
      "Average score of -10.573 @ 600/10000\n",
      "Average score of 2.322 @ 700/10000\n",
      "Average score of 15.530 @ 800/10000\n",
      "Average score of 53.160 @ 900/10000\n",
      "Average score of 181.738 @ 1000/10000\n",
      "Average score of 48.277 @ 1100/10000\n",
      "Average score of 87.521 @ 1200/10000\n",
      "Average score of 197.366 @ 1300/10000\n",
      "Average score of 235.467 @ 1400/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "4/24\n",
      "{'n_hidden_neurons': 64, 'learning_rate': 0.0005, 'discount_factor': 0.99, 'max_buffer_size': 10000, 'batch_size': 32, 'n_episodes': 10000, 'exploration_chance_start': 1.0, 'exploration_chance_end': 1e-05, 'exploration_chance_decay': 0.995, 'target_update_freq': 10, 'finish_check_freq': 100, 'finish_score': 200, 'per_params': {'bias_factor_start': 0.5, 'bias_factor_end': 1.0, 'bias_increment': 0.01, 'priority_scale': 0.7, 'td_error_clamp': 50}}\n",
      "0\n",
      "Average score of -156.950 @ 100/10000\n",
      "Average score of -57.641 @ 200/10000\n",
      "Average score of 9.821 @ 300/10000\n",
      "Average score of 44.897 @ 400/10000\n",
      "Average score of 8.770 @ 500/10000\n",
      "Average score of 41.834 @ 600/10000\n",
      "Average score of 110.157 @ 700/10000\n",
      "Average score of 226.838 @ 800/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1\n",
      "Average score of -138.709 @ 100/10000\n",
      "Average score of -50.146 @ 200/10000\n",
      "Average score of -34.959 @ 300/10000\n",
      "Average score of 54.819 @ 400/10000\n",
      "Average score of 124.250 @ 500/10000\n",
      "Average score of 149.019 @ 600/10000\n",
      "Average score of 176.587 @ 700/10000\n",
      "Average score of 173.561 @ 800/10000\n",
      "Average score of 171.384 @ 900/10000\n",
      "Average score of 213.730 @ 1000/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2\n",
      "Average score of -146.326 @ 100/10000\n",
      "Average score of -61.222 @ 200/10000\n",
      "Average score of -58.554 @ 300/10000\n",
      "Average score of -37.815 @ 400/10000\n",
      "Average score of 37.208 @ 500/10000\n",
      "Average score of 94.810 @ 600/10000\n",
      "Average score of 158.055 @ 700/10000\n",
      "Average score of 183.004 @ 800/10000\n",
      "Average score of 130.435 @ 900/10000\n",
      "Average score of 125.812 @ 1000/10000\n",
      "Average score of 204.606 @ 1100/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "5/24\n",
      "{'n_hidden_neurons': 64, 'learning_rate': 0.0005, 'discount_factor': 0.99, 'max_buffer_size': 10000, 'batch_size': 32, 'n_episodes': 10000, 'exploration_chance_start': 1.0, 'exploration_chance_end': 1e-05, 'exploration_chance_decay': 0.995, 'target_update_freq': 20, 'finish_check_freq': 100, 'finish_score': 200, 'per_params': {'bias_factor_start': 0.5, 'bias_factor_end': 1.0, 'bias_increment': 0.01, 'priority_scale': 0.7, 'td_error_clamp': 50}}\n",
      "0\n",
      "Average score of -149.237 @ 100/10000\n",
      "Average score of -66.091 @ 200/10000\n",
      "Average score of -58.895 @ 300/10000\n",
      "Average score of -59.370 @ 400/10000\n",
      "Average score of -62.972 @ 500/10000\n",
      "Average score of -70.581 @ 600/10000\n",
      "Average score of -80.723 @ 700/10000\n",
      "Average score of -91.310 @ 800/10000\n",
      "Average score of -101.500 @ 900/10000\n",
      "Average score of -85.707 @ 1000/10000\n",
      "Average score of -47.683 @ 1100/10000\n",
      "Average score of -48.746 @ 1200/10000\n",
      "Average score of -26.320 @ 1300/10000\n",
      "Average score of -15.402 @ 1400/10000\n",
      "Average score of -20.044 @ 1500/10000\n",
      "Average score of -46.193 @ 1600/10000\n",
      "Average score of -38.806 @ 1700/10000\n",
      "Average score of -22.545 @ 1800/10000\n",
      "Average score of -28.386 @ 1900/10000\n",
      "Average score of -8.829 @ 2000/10000\n",
      "Average score of -27.041 @ 2100/10000\n",
      "Average score of -3.564 @ 2200/10000\n",
      "Average score of 21.314 @ 2300/10000\n",
      "Average score of 47.345 @ 2400/10000\n",
      "Average score of 28.632 @ 2500/10000\n",
      "Average score of 36.586 @ 2600/10000\n",
      "Average score of 65.719 @ 2700/10000\n",
      "Average score of 178.779 @ 2800/10000\n",
      "Average score of 238.172 @ 2900/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1\n",
      "Average score of -151.515 @ 100/10000\n",
      "Average score of -73.488 @ 200/10000\n",
      "Average score of -56.085 @ 300/10000\n",
      "Average score of -55.993 @ 400/10000\n",
      "Average score of -72.876 @ 500/10000\n",
      "Average score of -70.355 @ 600/10000\n",
      "Average score of -54.162 @ 700/10000\n",
      "Average score of -27.240 @ 800/10000\n",
      "Average score of 95.124 @ 900/10000\n",
      "Average score of 134.597 @ 1000/10000\n",
      "Average score of 163.279 @ 1100/10000\n",
      "Average score of 177.734 @ 1200/10000\n",
      "Average score of 145.777 @ 1300/10000\n",
      "Average score of 140.194 @ 1400/10000\n",
      "Average score of 118.165 @ 1500/10000\n",
      "Average score of 110.476 @ 1600/10000\n",
      "Average score of 110.747 @ 1700/10000\n",
      "Average score of 118.617 @ 1800/10000\n",
      "Average score of 184.550 @ 1900/10000\n",
      "Average score of 159.423 @ 2000/10000\n",
      "Average score of 199.015 @ 2100/10000\n",
      "Average score of 204.755 @ 2200/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2\n",
      "Average score of -123.682 @ 100/10000\n",
      "Average score of -71.062 @ 200/10000\n",
      "Average score of -43.980 @ 300/10000\n",
      "Average score of -27.886 @ 400/10000\n",
      "Average score of -45.919 @ 500/10000\n",
      "Average score of 1.491 @ 600/10000\n",
      "Average score of 6.014 @ 700/10000\n",
      "Average score of 45.129 @ 800/10000\n",
      "Average score of 97.312 @ 900/10000\n",
      "Average score of 34.039 @ 1000/10000\n",
      "Average score of 72.677 @ 1100/10000\n",
      "Average score of 19.548 @ 1200/10000\n",
      "Average score of 77.865 @ 1300/10000\n",
      "Average score of 111.140 @ 1400/10000\n",
      "Average score of 132.458 @ 1500/10000\n",
      "Average score of 138.306 @ 1600/10000\n",
      "Average score of 224.701 @ 1700/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "6/24\n",
      "{'n_hidden_neurons': 64, 'learning_rate': 0.0005, 'discount_factor': 0.99, 'max_buffer_size': 10000, 'batch_size': 64, 'n_episodes': 10000, 'exploration_chance_start': 1.0, 'exploration_chance_end': 1e-05, 'exploration_chance_decay': 0.995, 'target_update_freq': 10, 'finish_check_freq': 100, 'finish_score': 200, 'per_params': {'bias_factor_start': 0.5, 'bias_factor_end': 1.0, 'bias_increment': 0.01, 'priority_scale': 0.7, 'td_error_clamp': 50}}\n",
      "0\n",
      "Average score of -154.771 @ 100/10000\n",
      "Average score of -67.137 @ 200/10000\n",
      "Average score of -37.278 @ 300/10000\n",
      "Average score of 1.539 @ 400/10000\n",
      "Average score of 72.054 @ 500/10000\n",
      "Average score of 153.561 @ 600/10000\n",
      "Average score of 143.099 @ 700/10000\n",
      "Average score of 158.825 @ 800/10000\n",
      "Average score of 134.252 @ 900/10000\n",
      "Average score of 109.201 @ 1000/10000\n",
      "Average score of 128.670 @ 1100/10000\n",
      "Average score of 161.125 @ 1200/10000\n",
      "Average score of 157.065 @ 1300/10000\n",
      "Average score of 201.852 @ 1400/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1\n",
      "Average score of -134.688 @ 100/10000\n",
      "Average score of -50.225 @ 200/10000\n",
      "Average score of 25.980 @ 300/10000\n",
      "Average score of 79.434 @ 400/10000\n",
      "Average score of 125.005 @ 500/10000\n",
      "Average score of 203.240 @ 600/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2\n",
      "Average score of -134.365 @ 100/10000\n",
      "Average score of -39.427 @ 200/10000\n",
      "Average score of 48.729 @ 300/10000\n",
      "Average score of 112.818 @ 400/10000\n",
      "Average score of 159.814 @ 500/10000\n",
      "Average score of 146.840 @ 600/10000\n",
      "Average score of 162.132 @ 700/10000\n",
      "Average score of 159.663 @ 800/10000\n",
      "Average score of 189.171 @ 900/10000\n",
      "Average score of 247.928 @ 1000/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "7/24\n",
      "{'n_hidden_neurons': 64, 'learning_rate': 0.0005, 'discount_factor': 0.99, 'max_buffer_size': 10000, 'batch_size': 64, 'n_episodes': 10000, 'exploration_chance_start': 1.0, 'exploration_chance_end': 1e-05, 'exploration_chance_decay': 0.995, 'target_update_freq': 20, 'finish_check_freq': 100, 'finish_score': 200, 'per_params': {'bias_factor_start': 0.5, 'bias_factor_end': 1.0, 'bias_increment': 0.01, 'priority_scale': 0.7, 'td_error_clamp': 50}}\n",
      "0\n",
      "Average score of -142.762 @ 100/10000\n",
      "Average score of -64.892 @ 200/10000\n",
      "Average score of -45.350 @ 300/10000\n",
      "Average score of -15.964 @ 400/10000\n",
      "Average score of 3.422 @ 500/10000\n",
      "Average score of 23.515 @ 600/10000\n",
      "Average score of 128.811 @ 700/10000\n",
      "Average score of 139.840 @ 800/10000\n",
      "Average score of 113.424 @ 900/10000\n",
      "Average score of 151.049 @ 1000/10000\n",
      "Average score of 185.096 @ 1100/10000\n",
      "Average score of 184.675 @ 1200/10000\n",
      "Average score of 180.159 @ 1300/10000\n",
      "Average score of 178.341 @ 1400/10000\n",
      "Average score of 184.047 @ 1500/10000\n",
      "Average score of 233.005 @ 1600/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "1\n",
      "Average score of -145.404 @ 100/10000\n",
      "Average score of -66.354 @ 200/10000\n",
      "Average score of -20.441 @ 300/10000\n",
      "Average score of -9.258 @ 400/10000\n",
      "Average score of -4.067 @ 500/10000\n",
      "Average score of -51.490 @ 600/10000\n",
      "Average score of 7.043 @ 700/10000\n",
      "Average score of 13.030 @ 800/10000\n",
      "Average score of 27.593 @ 900/10000\n",
      "Average score of 118.092 @ 1000/10000\n",
      "Average score of 150.530 @ 1100/10000\n",
      "Average score of 188.463 @ 1200/10000\n",
      "Average score of 240.233 @ 1300/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n",
      "2\n",
      "Average score of -136.401 @ 100/10000\n",
      "Average score of -69.533 @ 200/10000\n",
      "Average score of -59.831 @ 300/10000\n",
      "Average score of -39.562 @ 400/10000\n",
      "Average score of -8.437 @ 500/10000\n",
      "Average score of 62.147 @ 600/10000\n",
      "Average score of 69.905 @ 700/10000\n",
      "Average score of 6.196 @ 800/10000\n",
      "Average score of -127.312 @ 900/10000\n",
      "Average score of -137.948 @ 1000/10000\n",
      "Average score of -183.563 @ 1100/10000\n",
      "Average score of -162.400 @ 1200/10000\n",
      "Average score of -82.956 @ 1300/10000\n",
      "Average score of -21.709 @ 1400/10000\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def get_param_combos(params):\n",
    "    to_iter_tuples = []\n",
    "    iters = 1\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, list):\n",
    "            to_iter_tuples.append((k, v))\n",
    "            iters *= len(v)\n",
    "    p, v = zip(*to_iter_tuples)\n",
    "    return iters, [list(zip(p, c)) for c in itertools.product(*v)]\n",
    "\n",
    "def generate_powerset(lst):\n",
    "    powerset = []\n",
    "    for length in range(len(lst) + 1):\n",
    "        for subset in itertools.combinations(lst, length):\n",
    "            powerset.append(list(subset))\n",
    "    return powerset\n",
    "\n",
    "def hyperparam_optim():\n",
    "    params = {\n",
    "    'n_hidden_neurons': [64, 128],\n",
    "    'learning_rate': [1e-3, 5e-4, 1e-5],\n",
    "    'discount_factor': 0.99,\n",
    "    'max_buffer_size': 10_000,\n",
    "    'batch_size': [32, 64],\n",
    "    'n_episodes': 10_000,\n",
    "    'exploration_chance_start': 1.0,\n",
    "    'exploration_chance_end': 1e-5,\n",
    "    'exploration_chance_decay': 0.995,\n",
    "    'target_update_freq': [10, 20],\n",
    "    'finish_check_freq': 100,\n",
    "    'finish_score': 200,\n",
    "    'per_params': {\n",
    "        'bias_factor_start': 0.5,\n",
    "        'bias_factor_end': 1.0,\n",
    "        'bias_increment': 1e-2,\n",
    "        'priority_scale': 0.7,\n",
    "        'td_error_clamp': 50 # 1/4 of target end score\n",
    "        }\n",
    "    }\n",
    "    modification_opts = generate_powerset(['double', 'duelling', 'per'])\n",
    "    n_param_combos, param_combos = get_param_combos(params)\n",
    "    for opt in modification_opts:\n",
    "        print(opt)\n",
    "        for i in range(n_param_combos):\n",
    "            print(f'{i}/{n_param_combos}')\n",
    "            for p, v in param_combos[i]:\n",
    "                params[p] = v\n",
    "            print(params)\n",
    "            for x in range(3):\n",
    "                print(x)\n",
    "                agent = train(params, opt, save_data=True)\n",
    "\n",
    "hyperparam_optim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
