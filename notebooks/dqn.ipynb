{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8606ea0-4d1b-48f4-93c2-78f2d42e86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config params\n",
    "N_HIDDEN_LAYERS = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "MAX_BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44d4b4b-3691-493f-bd6c-13fd57fe7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class DQN(Module):\n",
    "    def __init__(self,\n",
    "                 n_states: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden_layers: int\n",
    "                ):\n",
    "        super(DQN, self).__init__()\n",
    "        self.l1 = Linear(n_states, n_hidden_layers)\n",
    "        self.l2 = Linear(n_hidden_layers, n_hidden_layers)\n",
    "        self.l3 = Linear(n_hidden_layers, n_actions)\n",
    "\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        x = self.l1(state).relu()\n",
    "        x = self.l2(x).relu()\n",
    "        return self.l3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6ff3da-f558-4b19-8e2b-284cf19d2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "from numpy.typing import NDArray\n",
    "from typing import List, Tuple\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,\n",
    "                 max_buffer_size: int\n",
    "                ):\n",
    "        self.buffer = deque(maxlen=max_buffer_size)\n",
    "\n",
    "\n",
    "    def add(self,\n",
    "            state: NDArray[float32],\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            next: NDArray[float32],\n",
    "            terminal: bool\n",
    "           ):\n",
    "        self.buffer.append((state, action, reward, next, terminal))\n",
    "\n",
    "\n",
    "    def sample(self, batches: int) -> Tuple[NDArray[float32], List[int], List[float], NDArray[float32], List[bool]]:\n",
    "        samples = random.sample(self.buffer, batches)\n",
    "        return zip(*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dcf24b-68b3-45a8-a1d8-d6b9e397827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from torch.backends import mps\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import MSELoss\n",
    "import numpy as np\n",
    "\n",
    "def _get_torch_device() -> str:\n",
    "    if cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 n_states: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden_layers: int,\n",
    "                 learning_rate: float,\n",
    "                 discount_factor: float,\n",
    "                 max_buffer_size: int,\n",
    "                 batch_size: int\n",
    "                ):\n",
    "        self.device = torch.device(_get_torch_device())\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.qnet = DQN(n_states, n_actions, n_hidden_layers).to(self.device)\n",
    "\n",
    "        # self.optimiser = SGD(self.qnet.parameters(), lr=learning_rate)\n",
    "        self.optimiser = Adam(self.qnet.parameters(), lr=learning_rate)\n",
    "        self.replay_buffer = ReplayBuffer(max_buffer_size)\n",
    "        \n",
    "        self.target_qnet = DQN(n_states, n_actions, n_hidden_layers).to(self.device)\n",
    "        self.target_qnet.load_state_dict(self.qnet.state_dict())\n",
    "        self.target_qnet.eval()\n",
    "\n",
    "\n",
    "    def step(self,\n",
    "             state: NDArray[float32],\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next: NDArray[float32],\n",
    "             terminal: bool\n",
    "            ):\n",
    "        self.replay_buffer.add(state, action, reward, next, terminal)\n",
    "        if len(self.replay_buffer.buffer) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "\n",
    "    def act(self,\n",
    "            state: NDArray[float32],\n",
    "            exploration_chance: float\n",
    "           ) -> int:\n",
    "        if random.random() > exploration_chance:\n",
    "            self.qnet.eval()\n",
    "            state_tensor_batched = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            state = state_tensor_batched.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnet(state)\n",
    "            chosen_action = np.argmax(action_values.cpu().detach().numpy())\n",
    "            self.qnet.train()\n",
    "            return chosen_action\n",
    "        else:\n",
    "            return random.choice(np.arange(self.n_actions))\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        states, actions, rewards, nexts, terminals = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack(states)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "        nexts = torch.from_numpy(np.stack(nexts)).float().to(self.device)\n",
    "        terminals = torch.from_numpy(np.array(terminals)).float().to(self.device)\n",
    "\n",
    "        q_values = self.qnet(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        next_q_values = self.target_qnet(nexts).max(1)[0].detach()\n",
    "        expected_q_values = rewards + self.discount_factor * next_q_values * (1 - terminals)\n",
    "\n",
    "        loss = MSELoss()(q_values, expected_q_values)\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimiser.step()\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_qnet.load_state_dict(self.qnet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0d5318-7dc7-4eb3-98ed-d57b4f2e5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config params\n",
    "\n",
    "N_EPISODES = 10_000\n",
    "EXPLORATION_CHANCE_START = 1.0\n",
    "EXPLORATION_CHANCE_END = 1e-4\n",
    "EXPLORATION_CHANCE_DECAY = 0.995\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "FINISH_CHECK_FREQ = 100\n",
    "FINISH_SCORE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c01862-a447-4d40-908c-e2e4749e065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(n_states,\n",
    "                 n_actions,\n",
    "                 N_HIDDEN_LAYERS,\n",
    "                 LEARNING_RATE,\n",
    "                 DISCOUNT_FACTOR,\n",
    "                 MAX_BUFFER_SIZE,\n",
    "                 BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dbcb6ee-063b-4633-b1c7-f062f5f2608c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/git/rl-lunar-lander/.venv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score of -120.216 @ 100/10000\n",
      "Average score of -49.519 @ 200/10000\n",
      "Average score of -21.419 @ 300/10000\n",
      "Average score of 39.940 @ 400/10000\n",
      "Average score of 29.926 @ 500/10000\n",
      "Average score of -41.250 @ 600/10000\n",
      "Average score of 83.400 @ 700/10000\n",
      "Average score of 143.569 @ 800/10000\n",
      "Average score of 149.752 @ 900/10000\n",
      "Average score of 181.229 @ 1000/10000\n",
      "Average score of 80.470 @ 1100/10000\n",
      "Average score of 211.658 @ 1200/10000\n",
      "Average score was above 200 over last 100 episodes. Ending training...\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "import random\n",
    "\n",
    "scores = []\n",
    "latest_scores = deque(maxlen=FINISH_CHECK_FREQ)\n",
    "\n",
    "exploration_chance = EXPLORATION_CHANCE_START\n",
    "\n",
    "for episode_n in range(1, N_EPISODES + 1):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state, exploration_chance)\n",
    "        next, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        terminal = terminated or truncated\n",
    "        agent.step(state, action, reward, next, terminal)\n",
    "        \n",
    "        state = next\n",
    "        score += reward\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    scores.append(score)\n",
    "    latest_scores.append(score)\n",
    "\n",
    "    exploration_chance = max(EXPLORATION_CHANCE_END, EXPLORATION_CHANCE_DECAY * exploration_chance)\n",
    "\n",
    "    if episode_n % TARGET_UPDATE_FREQ == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    if episode_n % FINISH_CHECK_FREQ == 0:\n",
    "        print(f'Average score of {np.mean(latest_scores):0.3f} @ {episode_n}/{N_EPISODES}')\n",
    "        if np.mean(latest_scores) >= FINISH_SCORE:\n",
    "            print(f'Average score was above {FINISH_SCORE} over last {FINISH_CHECK_FREQ} episodes. Ending training...')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba6c956-eae0-4234-8449-04f20854d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e16c897-da7d-4ef9-87f8-1eee2d6f5c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254.06528819038823\n"
     ]
    }
   ],
   "source": [
    "# visual testing\n",
    "\n",
    "human_env = gym.make('LunarLander-v2', render_mode='human')\n",
    "\n",
    "state, _ = human_env.reset()\n",
    "\n",
    "score = 0\n",
    "\n",
    "while True:\n",
    "    action = agent.act(state, 0)\n",
    "    state, reward, terminated, truncated, info = human_env.step(action)\n",
    "\n",
    "    terminal = terminated or truncated\n",
    "\n",
    "    score += reward\n",
    "\n",
    "    if terminal:\n",
    "        break\n",
    "\n",
    "print(score)\n",
    "\n",
    "human_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
