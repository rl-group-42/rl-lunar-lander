{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8606ea0-4d1b-48f4-93c2-78f2d42e86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config params\n",
    "N_HIDDEN_NODES = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "MAX_BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44d4b4b-3691-493f-bd6c-13fd57fe7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class DQN(Module):\n",
    "    def __init__(self,\n",
    "                 n_states: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden_nodes: int,\n",
    "                 duelling: bool = False\n",
    "                ):\n",
    "        super(DQN, self).__init__()\n",
    "        self.duelling = duelling\n",
    "        self.l1 = Linear(n_states, n_hidden_nodes)\n",
    "        self.l2 = Linear(n_hidden_nodes, n_hidden_nodes)\n",
    "        self.l3 = Linear(n_hidden_nodes, n_actions)\n",
    "        if duelling:\n",
    "            self.l4 = Linear(n_hidden_nodes, 1)\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        x = self.l1(state).relu()\n",
    "        x = self.l2(x).relu()\n",
    "        if self.duelling:\n",
    "            return self.l4(x) + (self.l3(x) - self.l3(x).max(dim=1, keepdim=True)[0])\n",
    "        else:\n",
    "            return self.l3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6ff3da-f558-4b19-8e2b-284cf19d2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "from numpy.typing import NDArray\n",
    "from typing import List, Tuple\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,\n",
    "                 max_buffer_size: int\n",
    "                ):\n",
    "        self.buffer = deque(maxlen=max_buffer_size)\n",
    "\n",
    "\n",
    "    def add(self,\n",
    "            state: NDArray[float32],\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            next: NDArray[float32],\n",
    "            terminal: bool\n",
    "           ):\n",
    "        self.buffer.append((state, action, reward, next, terminal))\n",
    "\n",
    "\n",
    "    def sample(self, batches: int) -> Tuple[NDArray[float32], List[int], List[float], NDArray[float32], List[bool]]:\n",
    "        samples = random.sample(self.buffer, batches)\n",
    "        return zip(*samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dcf24b-68b3-45a8-a1d8-d6b9e397827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from torch.backends import mps\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import MSELoss\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def _get_torch_device() -> str:\n",
    "    if cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 n_states: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden_nodes: int,\n",
    "                 learning_rate: float,\n",
    "                 discount_factor: float,\n",
    "                 max_buffer_size: int,\n",
    "                 batch_size: int,\n",
    "                 modifications: List[str] = None\n",
    "                ):\n",
    "        self.device = torch.device(_get_torch_device())\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.qnet = DQN(n_states, n_actions, n_hidden_nodes, 'duelling' in modifications).to(self.device)\n",
    "        self.modifications = modifications\n",
    "\n",
    "        # self.optimiser = SGD(self.qnet.parameters(), lr=learning_rate)\n",
    "        self.optimiser = Adam(self.qnet.parameters(), lr=learning_rate)\n",
    "        self.replay_buffer = ReplayBuffer(max_buffer_size)\n",
    "        self.target_qnet = DQN(n_states, n_actions, n_hidden_nodes, 'duelling' in modifications).to(self.device)\n",
    "        self.target_qnet.load_state_dict(self.qnet.state_dict())\n",
    "        self.target_qnet.eval()\n",
    "\n",
    "\n",
    "    def step(self,\n",
    "             state: NDArray[float32],\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next: NDArray[float32],\n",
    "             terminal: bool\n",
    "            ):\n",
    "        self.replay_buffer.add(state, action, reward, next, terminal)\n",
    "        if len(self.replay_buffer.buffer) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "\n",
    "    def act(self,\n",
    "            state: NDArray[float32],\n",
    "            exploration_chance: float\n",
    "           ) -> int:\n",
    "        if random.random() > exploration_chance:\n",
    "            self.qnet.eval()\n",
    "            state_tensor_batched = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            state = state_tensor_batched.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnet(state)\n",
    "            chosen_action = np.argmax(action_values.cpu().detach().numpy())\n",
    "            self.qnet.train()\n",
    "            return chosen_action\n",
    "        else:\n",
    "            return random.choice(np.arange(self.n_actions))\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        states, actions, rewards, nexts, terminals = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack(states)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "        nexts = torch.from_numpy(np.stack(nexts)).float().to(self.device)\n",
    "        terminals = torch.from_numpy(np.array(terminals)).float().to(self.device)\n",
    "\n",
    "        q_values = self.qnet(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        if 'double' in self.modifications:\n",
    "            next_q_values = self.target_qnet(nexts).gather(1, self.qnet(nexts).max(1)[1].unsqueeze(-1)).detach().squeeze(-1)\n",
    "        else:\n",
    "            next_q_values = self.target_qnet(nexts).max(1)[0].detach()\n",
    "\n",
    "        expected_q_values = rewards + self.discount_factor * next_q_values * (1 - terminals)\n",
    "\n",
    "        loss = MSELoss()(q_values, expected_q_values)\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimiser.step()\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_qnet.load_state_dict(self.qnet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0d5318-7dc7-4eb3-98ed-d57b4f2e5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config params\n",
    "\n",
    "N_EPISODES = 10_000\n",
    "EXPLORATION_CHANCE_START = 1.0\n",
    "EXPLORATION_CHANCE_END = 1e-4\n",
    "EXPLORATION_CHANCE_DECAY = 0.995\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "FINISH_CHECK_FREQ = 100\n",
    "FINISH_SCORE = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a73df0e5-4d62-47d0-af7f-851f58459f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score of -145.531 @ 100/10000\n",
      "Average score of -68.686 @ 200/10000\n",
      "Average score of 10.673 @ 300/10000\n",
      "Average score of 30.761 @ 400/10000\n",
      "Average score of 110.904 @ 500/10000\n",
      "Average score of 73.490 @ 600/10000\n",
      "Average score of 111.724 @ 700/10000\n",
      "Average score of 145.137 @ 800/10000\n",
      "Average score of 55.154 @ 900/10000\n",
      "Average score of 172.880 @ 1000/10000\n",
      "Average score of 199.781 @ 1100/10000\n",
      "Average score of 188.393 @ 1200/10000\n",
      "Average score of 187.736 @ 1300/10000\n",
      "Average score of 108.024 @ 1400/10000\n",
      "Average score of -22.186 @ 1500/10000\n",
      "Average score of 185.594 @ 1600/10000\n",
      "Average score of 156.228 @ 1700/10000\n",
      "Average score of 11.531 @ 1800/10000\n",
      "Average score of 92.326 @ 1900/10000\n",
      "Average score of 204.403 @ 2000/10000\n",
      "Average score of 209.802 @ 2100/10000\n",
      "Average score of 137.123 @ 2200/10000\n",
      "Average score of 168.348 @ 2300/10000\n",
      "Average score of 196.342 @ 2400/10000\n",
      "Average score of 224.495 @ 2500/10000\n",
      "Average score of 253.712 @ 2600/10000\n",
      "Average score was above 250 over last 100 episodes. Ending training...\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "def train(modifications) -> DQNAgent:\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(n_states,\n",
    "                     n_actions,\n",
    "                     N_HIDDEN_NODES,\n",
    "                     LEARNING_RATE,\n",
    "                     DISCOUNT_FACTOR,\n",
    "                     MAX_BUFFER_SIZE,\n",
    "                     BATCH_SIZE,\n",
    "                     modifications=modifications\n",
    "                    )\n",
    "\n",
    "    scores = []\n",
    "    latest_scores = deque(maxlen=FINISH_CHECK_FREQ)\n",
    "    \n",
    "    exploration_chance = EXPLORATION_CHANCE_START\n",
    "\n",
    "    for episode_n in range(1, N_EPISODES + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "    \n",
    "        while True:\n",
    "            action = agent.act(state, exploration_chance)\n",
    "            next, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            terminal = terminated or truncated\n",
    "            agent.step(state, action, reward, next, terminal)\n",
    "            \n",
    "            state = next\n",
    "            score += reward\n",
    "    \n",
    "            if terminal:\n",
    "                break\n",
    "    \n",
    "        scores.append(score)\n",
    "        latest_scores.append(score)\n",
    "    \n",
    "        exploration_chance = max(EXPLORATION_CHANCE_END, EXPLORATION_CHANCE_DECAY * exploration_chance)\n",
    "    \n",
    "        if episode_n % TARGET_UPDATE_FREQ == 0:\n",
    "            agent.update_target_network()\n",
    "    \n",
    "        if episode_n % FINISH_CHECK_FREQ == 0:\n",
    "            print(f'Average score of {np.mean(latest_scores):0.3f} @ {episode_n}/{N_EPISODES}')\n",
    "            if np.mean(latest_scores) >= FINISH_SCORE:\n",
    "                print(f'Average score was above {FINISH_SCORE} over last {FINISH_CHECK_FREQ} episodes. Ending training...')\n",
    "                break\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "agent = train(['double', 'duelling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d5aa4-41ed-427c-bfb7-75215c53efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_run_agent(agent):\n",
    "    env = gym.make('LunarLander-v2', render_mode='human')\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state, 0)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "    \n",
    "        terminal = terminated or truncated\n",
    "    \n",
    "        score += reward\n",
    "    \n",
    "        if terminal:\n",
    "            break\n",
    "    \n",
    "    print(f'Score achieved on test: {score}')\n",
    "    print(f'Steps taken until termination: {steps}')\n",
    "    env.close()\n",
    "\n",
    "visual_run_agent(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
