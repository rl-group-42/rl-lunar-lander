{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8606ea0-4d1b-48f4-93c2-78f2d42e86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config params\n",
    "N_HIDDEN_NODES = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "MAX_BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44d4b4b-3691-493f-bd6c-13fd57fe7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class DQN(Module):\n",
    "    def __init__(self,\n",
    "                 n_states: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden_nodes: int,\n",
    "                 duelling: bool = False\n",
    "                ):\n",
    "        super(DQN, self).__init__()\n",
    "        self.duelling = duelling\n",
    "        self.l1 = Linear(n_states, n_hidden_nodes)\n",
    "        self.l2 = Linear(n_hidden_nodes, n_hidden_nodes)\n",
    "        self.l3 = Linear(n_hidden_nodes, n_actions)\n",
    "        if duelling:\n",
    "            self.l4 = Linear(n_hidden_nodes, 1) # state value estimation\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        x = self.l1(state).relu()\n",
    "        x = self.l2(x).relu()\n",
    "        if self.duelling:\n",
    "            return self.l4(x) + (self.l3(x) - self.l3(x).mean(dim=1, keepdim=True)[0])\n",
    "        else:\n",
    "            return self.l3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6ff3da-f558-4b19-8e2b-284cf19d2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "from numpy.typing import NDArray\n",
    "from typing import List, Tuple\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,\n",
    "                 max_buffer_size: int\n",
    "                ):\n",
    "        self.buffer = deque(maxlen=max_buffer_size)\n",
    "\n",
    "\n",
    "    def add(self,\n",
    "            state: NDArray[float32],\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            next: NDArray[float32],\n",
    "            terminal: bool\n",
    "           ):\n",
    "        self.buffer.append((state, action, reward, next, terminal))\n",
    "\n",
    "\n",
    "    def sample(self, \n",
    "               batches: int\n",
    "              ) -> Tuple[NDArray[float32], List[int], List[float], NDArray[float32], List[bool]]:\n",
    "        samples = random.sample(self.buffer, batches)\n",
    "        return zip(*samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc043ddd-da8e-4fbc-a796-3e06a5b1bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PrioritisedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self,\n",
    "                 max_buffer_size: int,\n",
    "                 bias_factor_start: float = 0.4,\n",
    "                 bias_factor_end: float = 1.0,\n",
    "                 bias_increment: float = 0.01,\n",
    "                 priority_scale: float = 0.6\n",
    "                ):\n",
    "        super(PrioritisedReplayBuffer, self).__init__(max_buffer_size)\n",
    "        self.priorities = deque(maxlen=max_buffer_size)\n",
    "        self.min_priority = 0.01\n",
    "        self.bias_factor = bias_factor_start\n",
    "        self.bias_factor_end = bias_factor_end\n",
    "        self.bias_increment = bias_increment\n",
    "        self.priority_scale = priority_scale\n",
    "\n",
    "    def add(self,\n",
    "            state: NDArray[float32],\n",
    "            action: int,\n",
    "            reward: float,\n",
    "            next: NDArray[float32],\n",
    "            terminal: bool,\n",
    "            error: float\n",
    "           ):\n",
    "        super(PrioritisedReplayBuffer, self).add(state, action, reward, next, terminal)\n",
    "        self.priorities.append((np.abs(error) + self.min_priority)**self.priority_scale)\n",
    "\n",
    "    def sample(self,\n",
    "               batches: int\n",
    "              ) -> Tuple[NDArray[float32], List[int], List[float], NDArray[float32], List[bool], List[int], List[float]]:\n",
    "        probas = np.array(self.priorities)/np.sum(self.priorities)\n",
    "        indices = np.random.choice(len(self.buffer), batches, p=probas)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        states, actions, rewards, nexts, terminals = zip(*samples)\n",
    "        weights = (1/len(self.buffer) * 1/probas[indices]) ** self.bias_factor\n",
    "        weights /= np.max(weights)\n",
    "        return states, actions, rewards, nexts, terminals, indices, weights\n",
    "\n",
    "    def update_priorities(self, idx, error):\n",
    "        self.priorities[idx] = (np.abs(error) + self.min_priority) ** self.priority_scale\n",
    "\n",
    "    def update_bias_factor(self):\n",
    "        self.bias_factor = min(self.bias_factor + self.bias_increment, self.bias_factor_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89dcf24b-68b3-45a8-a1d8-d6b9e397827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from torch.backends import mps\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import MSELoss\n",
    "from typing import List, Dict\n",
    "\n",
    "def _get_torch_device() -> str:\n",
    "    if cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 n_states: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden_nodes: int,\n",
    "                 learning_rate: float,\n",
    "                 discount_factor: float,\n",
    "                 max_buffer_size: int,\n",
    "                 batch_size: int,\n",
    "                 modifications: List[str] = None,\n",
    "                 per_params: Dict[str, float] = None\n",
    "                ):\n",
    "        self.device = torch.device(_get_torch_device())\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.qnet = DQN(n_states, n_actions, n_hidden_nodes, 'duelling' in modifications).to(self.device)\n",
    "        self.modifications = modifications\n",
    "\n",
    "        # self.optimiser = SGD(self.qnet.parameters(), lr=learning_rate)\n",
    "        self.optimiser = Adam(self.qnet.parameters(), lr=learning_rate)\n",
    "        if 'per' in modifications:\n",
    "            self.replay_buffer = PrioritisedReplayBuffer(max_buffer_size, **per_params)\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(max_buffer_size)\n",
    "        self.target_qnet = DQN(n_states, n_actions, n_hidden_nodes, 'duelling' in modifications).to(self.device)\n",
    "        self.target_qnet.load_state_dict(self.qnet.state_dict())\n",
    "        self.target_qnet.eval()\n",
    "\n",
    "    \n",
    "    def get_td_error(self,\n",
    "                     state: float32,\n",
    "                     action: int,\n",
    "                     reward: float,\n",
    "                     next: float32,\n",
    "                     terminal: bool\n",
    "                    ):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        action_tensor = torch.tensor([action], device=self.device)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        next_tensor = torch.from_numpy(next).float().unsqueeze(0).to(self.device)\n",
    "        terminal_tensor = torch.tensor([terminal], device=self.device)\n",
    "        curr_q = self.qnet(state_tensor).gather(1, action_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "        next_q = self.target_qnet(next_tensor).max(1)[0]\n",
    "        expected_q = reward_tensor + self.discount_factor * next_q * (1 - terminal_tensor)\n",
    "        return MSELoss()(current_q, expected_q).item()\n",
    "\n",
    "\n",
    "    def _step_no_per(self,\n",
    "             state: NDArray[float32],\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next: NDArray[float32],\n",
    "             terminal: bool\n",
    "            ):\n",
    "        self.replay_buffer.add(state, action, reward, next, terminal)\n",
    "        if len(self.replay_buffer.buffer) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "\n",
    "    def _step_per(self,\n",
    "             state: NDArray[float32],\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next: NDArray[float32],\n",
    "             terminal: bool\n",
    "            ):\n",
    "        error = self.get_td_error(state, action, reward, next, terminal)\n",
    "        self.replay_buffer.add(state, action, reward, next, terminal, error)\n",
    "        if len(self.replay_buffer.buffer) > self.batch_size:\n",
    "            self.update_model()\n",
    "\n",
    "\n",
    "    def step(self,\n",
    "             state: NDArray[float32],\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next: NDArray[float32],\n",
    "             terminal: bool\n",
    "            ):\n",
    "        if 'per' in self.modifications:\n",
    "            self._step_per(state, action, reward, next, terminal)\n",
    "        else:\n",
    "            self._step_no_per(state, action, reward, next, terminal)\n",
    "\n",
    "\n",
    "    def act(self,\n",
    "            state: NDArray[float32],\n",
    "            exploration_chance: float\n",
    "           ) -> int:\n",
    "        if random.random() > exploration_chance:\n",
    "            self.qnet.eval()\n",
    "            state_tensor_batched = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            state = state_tensor_batched.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnet(state)\n",
    "            chosen_action = np.argmax(action_values.cpu().detach().numpy())\n",
    "            self.qnet.train()\n",
    "            return chosen_action\n",
    "        else:\n",
    "            return random.choice(np.arange(self.n_actions))\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        if 'per' in self.modifications:\n",
    "            states, actions, rewards, nexts, terminals, indices, weights = self.replay_buffer.sample(self.batch_size)\n",
    "        else:\n",
    "            states, actions, rewards, nexts, terminals = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack(states)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "        nexts = torch.from_numpy(np.stack(nexts)).float().to(self.device)\n",
    "        terminals = torch.from_numpy(np.array(terminals)).float().to(self.device)\n",
    "\n",
    "        q_values = self.qnet(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        if 'double' in self.modifications:\n",
    "            next_q_values = self.target_qnet(nexts).gather(1, self.qnet(nexts).max(1)[1].unsqueeze(-1)).detach().squeeze(-1)\n",
    "        else:\n",
    "            next_q_values = self.target_qnet(nexts).max(1)[0].detach()\n",
    "\n",
    "        expected_q_values = rewards + self.discount_factor * next_q_values * (1 - terminals)\n",
    "\n",
    "        loss = MSELoss()(q_values, expected_q_values)\n",
    "        if 'per' in self.modifications:\n",
    "            loss = (loss * torch.from_numpy(weights).float().to(self.device)).mean()\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimiser.step()\n",
    "\n",
    "        if 'per' in self.modifications:\n",
    "            for i, (state,\n",
    "                      action,\n",
    "                      reward,\n",
    "                      next,\n",
    "                      terminal\n",
    "                     ) in enumerate(zip(states.cpu().numpy(),\n",
    "                                        actions.cpu().numpy(),\n",
    "                                        rewards.cpu().numpy(),\n",
    "                                        nexts.cpu().numpy(),\n",
    "                                        terminals.cpu().numpy())\n",
    "                                   ):\n",
    "                error = self.get_td_error(state, action, reward, next, terminal)\n",
    "                self.replay_buffer.update_priorities(i, error)\n",
    "            self.replay_buffer.update_bias_factor()\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_qnet.load_state_dict(self.qnet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0d5318-7dc7-4eb3-98ed-d57b4f2e5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config params\n",
    "\n",
    "N_EPISODES = 10_000\n",
    "EXPLORATION_CHANCE_START = 1.0\n",
    "EXPLORATION_CHANCE_END = 1e-4\n",
    "EXPLORATION_CHANCE_DECAY = 0.995\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "FINISH_CHECK_FREQ = 100\n",
    "FINISH_SCORE = 250\n",
    "PER_PARAMS = {\n",
    "     'bias_factor_start': 0.4,\n",
    "     'bias_factor_end': 1.0,\n",
    "     'bias_increment': 0.01,\n",
    "     'priority_scale': 0.6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a73df0e5-4d62-47d0-af7f-851f58459f81",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent\n\u001b[0;32m---> 58\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdouble\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mduelling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mper\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(modifications)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mnext\u001b[39m, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     33\u001b[0m terminal \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m---> 34\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m\n\u001b[1;32m     37\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[12], line 100\u001b[0m, in \u001b[0;36mDQNAgent.step\u001b[0;34m(self, state, action, reward, next, terminal)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m          state: NDArray[float32],\n\u001b[1;32m     94\u001b[0m          action: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m          terminal: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     98\u001b[0m         ):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodifications:\n\u001b[0;32m--> 100\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_per\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_no_per(state, action, reward, \u001b[38;5;28mnext\u001b[39m, terminal)\n",
      "Cell \u001b[0;32mIn[12], line 86\u001b[0m, in \u001b[0;36mDQNAgent._step_per\u001b[0;34m(self, state, action, reward, next, terminal)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step_per\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     80\u001b[0m          state: NDArray[float32],\n\u001b[1;32m     81\u001b[0m          action: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m          terminal: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     85\u001b[0m         ):\n\u001b[0;32m---> 86\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_td_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd(state, action, reward, \u001b[38;5;28mnext\u001b[39m, terminal, error)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n",
      "Cell \u001b[0;32mIn[12], line 63\u001b[0m, in \u001b[0;36mDQNAgent.get_td_error\u001b[0;34m(self, state, action, reward, next, terminal)\u001b[0m\n\u001b[1;32m     61\u001b[0m curr_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnet(state_tensor)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m next_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_qnet(next_tensor)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 63\u001b[0m expected_q \u001b[38;5;241m=\u001b[39m reward_tensor \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m next_q \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mterminal_tensor\u001b[49m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MSELoss()(current_q, expected_q)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/git/rl-lunar-lander/.venv/lib/python3.9/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/git/rl-lunar-lander/.venv/lib/python3.9/site-packages/torch/_tensor.py:909\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "def train(modifications) -> DQNAgent:\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(n_states,\n",
    "                     n_actions,\n",
    "                     N_HIDDEN_NODES,\n",
    "                     LEARNING_RATE,\n",
    "                     DISCOUNT_FACTOR,\n",
    "                     MAX_BUFFER_SIZE,\n",
    "                     BATCH_SIZE,\n",
    "                     modifications=modifications,\n",
    "                     per_params=PER_PARAMS if 'per' in modifications else None\n",
    "                    )\n",
    "\n",
    "    scores = []\n",
    "    latest_scores = deque(maxlen=FINISH_CHECK_FREQ)\n",
    "    \n",
    "    exploration_chance = EXPLORATION_CHANCE_START\n",
    "\n",
    "    for episode_n in range(1, N_EPISODES + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "    \n",
    "        while True:\n",
    "            action = agent.act(state, exploration_chance)\n",
    "            next, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            terminal = terminated or truncated\n",
    "            agent.step(state, action, reward, next, terminal)\n",
    "            \n",
    "            state = next\n",
    "            score += reward\n",
    "    \n",
    "            if terminal:\n",
    "                break\n",
    "    \n",
    "        scores.append(score)\n",
    "        latest_scores.append(score)\n",
    "    \n",
    "        exploration_chance = max(EXPLORATION_CHANCE_END, EXPLORATION_CHANCE_DECAY * exploration_chance)\n",
    "    \n",
    "        if episode_n % TARGET_UPDATE_FREQ == 0:\n",
    "            agent.update_target_network()\n",
    "    \n",
    "        if episode_n % FINISH_CHECK_FREQ == 0:\n",
    "            print(f'Average score of {np.mean(latest_scores):0.3f} @ {episode_n}/{N_EPISODES}')\n",
    "            if np.mean(latest_scores) >= FINISH_SCORE:\n",
    "                print(f'Average score was above {FINISH_SCORE} over last {FINISH_CHECK_FREQ} episodes. Ending training...')\n",
    "                break\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "agent = train(['double', 'duelling', 'per'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d5aa4-41ed-427c-bfb7-75215c53efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_run_agent(agent):\n",
    "    env = gym.make('LunarLander-v2', render_mode='human')\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state, 0)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "    \n",
    "        terminal = terminated or truncated\n",
    "    \n",
    "        score += reward\n",
    "    \n",
    "        if terminal:\n",
    "            break\n",
    "    \n",
    "    print(f'Score achieved on test: {score}')\n",
    "    print(f'Steps taken until termination: {steps}')\n",
    "    env.close()\n",
    "\n",
    "visual_run_agent(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
